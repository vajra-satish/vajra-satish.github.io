{
  "hash": "e0d79b80d166c2fd679dd3c9d929e5a9",
  "result": {
    "markdown": "---\ntitle: \"Comparison Between Treatment and Control Group: Regression Analysis\"\nauthor: \"Satish Bajracharya\"\ndate: \"2024-03-18\"\ncategories: [RCT]\nimage: \"image.jpg\"\nengine: \"knitr\"\n---\n\n::: {.cell}\n<style type=\"text/css\">\np {\n  text-align: justify\n}\n</style>\n:::\n\n\n# Making comparisons between treatment and control group\n\nWhile measuring the impact of an intervention or a program, we randomly assign individuals to treatment and control groups and compare the average outcome in each group. In general, we ask if the outcomes are different between the groups. \n\n::: {.callout-note}\nResearchers use regression analysis to report the results of a RCT in scientific journals or reports.\n\nRegression equation: $y_i= \\alpha + \\beta T_i + \\epsilon_i$. Here, the outcome variable is denoted by $y$, error term is deonted by $\\epsilon_i$, and treatment variable is denoted by $T_i$. If an individual is assigned to the treatment group, $T = 1$, otherwise $T = 0$.\n:::\n\nA regression not just allows us to quantify the relationship between the treatment and outcome variable but also quantify the associations between outcomes and other variables. When we introduce other variables, we say that we are controlling for other factors. \n\n$$y_i = \\alpha + \\beta T_i + \\gamma_1 Income_i + \\gamma_2 Education_i + ... $$\n\nHere, we are controlling for income and education in regression equation. Introducing such controls may or may not be useful (a separate blog on control variables will be uploaded later).  \n\n# Measuring the average impact of an intervention?\n\nTo measure the average impact of an intervention, we estimate the average outcome in the treatment group, $\\bar y (T=1)$, and compare it with the average outcome in the control group, $\\bar y (T=0)$. \n\n##### Computing average outcome for the control group and treatment group\n\nLets compute the average outcome for the treatment and control group. For the treatment group, the treatment variable will take on the value of one. \n\n$$ \\bar y(1) = \\alpha + \\beta * 1 = \\alpha + \\beta $$\nThe average outcome of the treatment group is the sum of coefficients $\\alpha$ and $\\beta$\n\nFor the control group, the treatment variable will take on the value of zero. \n\n$$\\bar y(0) = \\alpha + \\beta * 0 = \\alpha$$\n\nThe average outcome of the control group is $\\alpha$, which is also the intercept term. \n\nWe can calculate the average impact of an intervention by calculating the difference in average outcome between the treatment and control group. \n\n$$\\bar y(1) - \\bar y(0) = \\alpha + \\beta -\\alpha = \\beta$$\n\n# Population parameters and their estimates\n\nIn a regression equation, the parameter $\\hat{\\beta}$ gives an estimate of the average impact of an intervention. We estimate the true population parameter $\\beta$ from a sample in a regression analysis. Hence, we use $\\hat{\\beta}$ instead of $\\beta$ to denote that it is an estimate of the population parameter. We do the same for the parameter $\\alpha$. Since these coefficients are estimates, the standard error is also reported in the regression output. \n\n::: {.callout-tip}\n## Standard error\n\nThe standard error is a measure of how uncertain we are about the true underlying value of a coefficient.\n:::\n\nThe $\\beta$ coefficient is the difference in the averages of the treatment and control group. The difference is just an estimate of the difference in the true underlying mean. \n\n$$True \\space parameter = \\beta \\approx Estimated \\space parameter = \\hat{\\beta}$$\n$$\\beta = \\mu_1 - \\mu_0 \\approx \\hat{\\beta} = \\bar{y} (1) - \\bar{y}(0)$$\n\nSince the coefficient $\\hat{\\beta}$ is an estimate of the true underlying mean, we could form a hypothesis about the true difference in mean, i.e., $H_0: \\mu_1 = \\mu_0$ against $H_A: \\mu_1 \\neq \\mu_0$. We can test the hypothesis about $\\beta$ by forming confidence intervals around $\\hat{\\beta}$ and by calculating the corresponding p-values. \n\n## Decision rules\n\nThe $H_0$ is the benchmark against which virtually all interventions are measured. The test will yield a **confidence interval** and a **p-value**. \n\n::: {.callout-note}\nThe confidence interval gives a range of values which likely includes the true population parameter at a given confidence level.\n\nThe p-value is a probability that measures how compatible the data are with the null hypothesis. It is the probability of observing the data as extreme as what we observed, assuming that the $H_0$ is true.\n:::\n\nConfidence interval\n\n- If the null value falls within the range of the confidence interval, we fail to reject the $H_0$.\n- If the null value falls outside the confidence interval, we reject the $H_0$.\n\n::: {.callout-tip}\n## Sample size and confidence interval\n\nIncreasing the sample size will shrink the confidence interval, which results in an improvement in the precision of our estimates. The increase in the precision improves the likelihood of detecting the impact of an intervention.\n:::\n\nP-value\n\n- If the p-value is less than the significance level, we reject the $H_0$.\n- If the p-value is more than the significance level, we fail reject the $H_0$.\n\n# Things to remember while conducting RCT\n\n- Controlling for additional variables\n- Heterogeneous treatment effects\n- Spillover effects\n- Imperfect compliance and attrition\n\n::: {.callout-note}\nMore on these topics will be uploaded later.\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}